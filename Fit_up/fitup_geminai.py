import os
from fastapi import FastAPI
from dotenv import load_dotenv
from langchain_core.output_parsers import JsonOutputParser
from langchain_google_genai import ChatGoogleGenerativeAI
from IOstruct import QuestInput, QuestOutput, StatInput, StatOutput
from prompt import quest_prompt, status_prompt

load_dotenv()
GOOGLE_GEMINI_API_KEY = os.getenv("GOOGLE_GEMINI_API_KEY")
app = FastAPI()

# model 
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", api_key=GOOGLE_GEMINI_API_KEY) # gemini-1.5-flash / gemini-pro

# ì¶œë ¥ ê²°ê³¼ ì¤‘ ë¬¸ìì—´ì„ ì„ ë³„
quest_output_parser = JsonOutputParser(pydantic_object=QuestOutput)

# chain ì—°ê²° (LCEL)
quest = quest_prompt | llm | quest_output_parser

@app.post("/generate-quest"  """, response_model=QuestOutput""") #output parserë¥¼ ì§€ì •í–ˆì§€ë§Œ í•œë²ˆë” ê²€ì¦í•˜ëŠ” ìš©ë„
async def generate_quest_endpoint(input_data: QuestInput):
    result = quest.invoke({
        "input_data": input_data.model_dump()  # QuestInput â†’ dict
    })
    return result

# stat ëª¨ë¸

status_output_parser = JsonOutputParser(pydantic_object=StatOutput)

status = status_prompt | llm | status_output_parser

@app.post("/stats"   """, response_model = StatOutput""")
async def compute_stats(input_data: StatInput):
    result = status.invoke({
        'input_data': input_data.model_dump()
    })
    return result
# @app.post("/stats", response_model=StatOutput)
# async def compute_stats(input_data: StatInput):
#     try:
#         print("ğŸ“¥ ë°›ì€ ì…ë ¥:", input_data)
#         result = status.invoke({
#             'input_data': input_data.model_dump()
#         })
#         print("ğŸ“¤ LLM ì‘ë‹µ ê²°ê³¼:", result)
#         return result
#     except Exception as e:
#         print("âŒ ì˜ˆì™¸ ë°œìƒ:", str(e))
#         return {"error": str(e)}  # ì„ì‹œ JSON ë°˜í™˜
